{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/adityaan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/adityaan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/adityaan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adityaan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import gensim.downloader as gs\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_HTML = re.compile('<.*?>')            # Regex to match HTML tags\n",
    "CLEAN_URL = re.compile('(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "CLEAN_SPACES = re.compile('\\s+')            # Regex to match multiple spaces\n",
    "CLEAN_NON_ALPHA = re.compile('[^a-zA-Z]')   # Regex to match non-alphabetic characters\n",
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.y[index] == 'class_1':\n",
    "            # label = torch.Tensor([1, 0, 0])\n",
    "            label = 0\n",
    "        elif self.y[index] == 'class_2':\n",
    "            # label = torch.Tensor([0, 1, 0])\n",
    "            label = 1\n",
    "        elif self.y[index] == 'class_3':\n",
    "            # label = torch.Tensor([0, 0, 1])\n",
    "            label = 2\n",
    "        return torch.from_numpy(self.X[index]).type(torch.float32), label\n",
    "\n",
    "\n",
    "def fit_model_cv(pipeline, parameter_grid, training_data, target_labels):\n",
    "    cv = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=parameter_grid,\n",
    "    )\n",
    "    cv.fit(training_data, target_labels)\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_model_metrics(model, testing_data, testing_labels):\n",
    "    y_pred = model.predict(testing_data)\n",
    "    accuracy = accuracy_score(testing_labels, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader, \n",
    "    val_dataloader,\n",
    "    num_epochs=30,\n",
    "    lr_scheduler=None\n",
    "    ):    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        metrics = {\n",
    "            'train_acc': 0,\n",
    "            'train_loss': 0.0,\n",
    "            'val_acc': 0,\n",
    "            'val_loss': 0.0\n",
    "        }\n",
    "\n",
    "        for i, (X, y) in enumerate(tqdm(train_dataloader)):\n",
    "            model.train()\n",
    "            # Zero optim gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print(f\"{torch.argmax(outputs, axis=1) = }\")\n",
    "            # print(f\"{y = }\")\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            metrics['train_acc'] += (torch.argmax(outputs, axis=1) == y).float().sum()\n",
    "            \n",
    "            # Calculate the loss\n",
    "            metrics['train_loss'] += loss\n",
    "        \n",
    "        metrics['train_acc'] /= (len(train_dataloader) * train_dataloader.batch_size)\n",
    "        metrics['train_loss'] /= (len(train_dataloader) * train_dataloader.batch_size)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        for i, (X, y) in enumerate(tqdm(val_dataloader)):\n",
    "            model.eval()\n",
    "            \n",
    "            # Move to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            metrics['val_acc'] += (torch.argmax(outputs, axis=1) == y).float().sum()\n",
    "            \n",
    "            # Calculate the loss\n",
    "            metrics['val_loss'] += loss\n",
    "\n",
    "            \n",
    "        metrics['val_acc'] /= (len(val_dataloader) * val_dataloader.batch_size)\n",
    "        metrics['val_loss'] /= (len(val_dataloader) * val_dataloader.batch_size)\n",
    "        \n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n",
    "        print(\"Mode\\tLoss\\tAcc\")\n",
    "        print(f\"Train\\t{metrics['train_loss']:.2f}\\t{metrics['train_acc']:.2f}\")\n",
    "        print(f\"Valid\\t{metrics['val_loss']:.2f}\\t{metrics['val_acc']:.2f}\")\n",
    "        \n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "def bin_column(column, bins, labels):\n",
    "    \"\"\"\n",
    "    :param column: pd.Series\n",
    "    :param bins: list\n",
    "    :param labels: list\n",
    "    :return: pd.Series\n",
    "    \"\"\"\n",
    "\n",
    "    bins.insert(0, -float('inf'))\n",
    "\n",
    "    # Use pd.IntervalIndex to create bins to split the data\n",
    "    bins = pd.IntervalIndex.from_breaks(bins)\n",
    "\n",
    "    print(bins)\n",
    "\n",
    "    x = pd.cut(column, bins=bins, include_lowest=True)\n",
    "    x = x.cat.rename_categories(labels)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    # Convert ratings to numeric\n",
    "    # Ignore ratings that are not numerals\n",
    "    dataframe['star_rating_numeric'] = pd.to_numeric(dataframe.star_rating, errors='coerce')\n",
    "\n",
    "    # Drop NaN\n",
    "    dataframe.dropna(inplace=True)\n",
    "    \n",
    "    # Consider reviews that have 50 or more characters\n",
    "    dataframe = dataframe[dataframe.review_body.apply(lambda x: len(x)) > 150]\n",
    "\n",
    "    # Bin ratings into 3 classes\n",
    "    # 1 and 2   class_1\n",
    "    # 3         class_2\n",
    "    # 4 and 5   class_3\n",
    "\n",
    "    dataframe['target'] = bin_column(dataframe.star_rating_numeric,\n",
    "                                     [2, 3, 5],\n",
    "                                     labels=['class_1', 'class_2', 'class_3'])\n",
    "\n",
    "    # In the interest of computational simplicity,\n",
    "    # keep only 20000 instances of each class\n",
    "\n",
    "    tiny_df = pd.DataFrame(\n",
    "        columns=['star_rating', 'review_body', 'star_rating_numeric'])\n",
    "\n",
    "    for cls in dataframe.target.unique():\n",
    "        tiny_df = pd.concat([\n",
    "            tiny_df,\n",
    "            dataframe[dataframe.target == cls].sample(20000, random_state=42)\n",
    "        ])\n",
    "\n",
    "    return tiny_df\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML and URL tags from text\n",
    "    text = re.sub(CLEAN_HTML, ' ', text)\n",
    "    text = re.sub(CLEAN_URL, ' ', text)\n",
    "\n",
    "    # Perform contractions on the text\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(CLEAN_NON_ALPHA, ' ', text)\n",
    "\n",
    "    # Remove additional spaces\n",
    "    text = re.sub(CLEAN_SPACES, ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_sentence_embedding(sentence,\n",
    "                           wv_model, \n",
    "                           return_type='average', \n",
    "                           num_words=10,\n",
    "                           flatten=False):\n",
    "    # Split the sentence on space to get individual words\n",
    "    words = sentence.split(' ')\n",
    "\n",
    "    if return_type == 'average':\n",
    "        sentence_encoding = np.zeros((wv_model.vector_size))\n",
    "        count = 0\n",
    "        for word in words:\n",
    "            try:\n",
    "                sentence_encoding += wv_model[word]\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if count != 0:\n",
    "            return sentence_encoding / count\n",
    "        else:\n",
    "            return np.zeros((wv_model.vector_size))\n",
    "            # return 'NA'\n",
    "\n",
    "    elif return_type == 'truncate':\n",
    "        sentence_encoding = np.zeros((wv_model.vector_size, num_words))\n",
    "        count = 0\n",
    "        idx = 0\n",
    "\n",
    "        # Consider only first 'n' words\n",
    "        while idx < min(num_words, len(words)) and count < len(words):\n",
    "            try:\n",
    "                sentence_encoding[:, idx] = wv_model[words[count]]\n",
    "                idx += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            count += 1\n",
    "        \n",
    "        if flatten:\n",
    "            # Reshaping to a column vector in advance instead of flattening later\n",
    "            return sentence_encoding.reshape(-1)\n",
    "        else:\n",
    "            return sentence_encoding.T\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size, 100)\n",
    "        self.hidden_layer = nn.Linear(100, 10)\n",
    "        self.output_layer = nn.Linear(10, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 output_size,\n",
    "                 rnn_layer_type='rnn'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_hidden_size = hidden_size\n",
    "        self.rnn_num_layers = num_layers\n",
    "        self.rnn_layer_type = rnn_layer_type.lower()\n",
    "\n",
    "        if rnn_layer_type.lower() == 'rnn':\n",
    "            self.rnn = nn.RNN(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "        elif rnn_layer_type.lower() == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "        elif rnn_layer_type.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('Only rnn, lstm or gru supported for rnn_layer_type')\n",
    "\n",
    "        self.hidden_layer = nn.Linear(hidden_size, 10)\n",
    "        self.out_layer = nn.Linear(10, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.rnn_layer_type == 'rnn':\n",
    "            _, h = self.rnn(x)\n",
    "        elif self.rnn_layer_type == 'lstm':\n",
    "            _, (h, _) = self.rnn(x)\n",
    "        elif self.rnn_layer_type == 'gru':\n",
    "            _, h = self.rnn(x)\n",
    "        \n",
    "        h = h[-1, :, :]\n",
    "        x = self.hidden_layer(h)\n",
    "        x = self.relu(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data_og = pd.read_csv(filepath,\n",
    "                          delimiter='\\t',\n",
    "                          usecols=['star_rating', 'review_body'],\n",
    "                          on_bad_lines='skip',)\n",
    "    \n",
    "    # Prepare the dataset\n",
    "    # Select reviews with more than 150 characters\n",
    "    # Select 60000 reviews from each class\n",
    "    tiny_df = prepare_data(data_og.copy())\n",
    "    \n",
    "    # Clean data\n",
    "    tqdm.pandas()\n",
    "    print(f\"Average length of review before cleaning: {tiny_df.review_body.apply(len).mean()}\")\n",
    "    tiny_df['review_body_pp'] = tiny_df.review_body.progress_apply(clean_text)\n",
    "    print(f\"Average length of review before cleaning: {tiny_df.review_body_pp.apply(len).mean()}\")\n",
    "    \n",
    "    return tiny_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalIndex([(-inf, 2.0], (2.0, 3.0], (3.0, 5.0]], dtype='interval[float64, right]')\n",
      "Average length of review before cleaning: 437.1336166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:05<00:00, 10287.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of review before cleaning: 421.1295\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating_numeric</th>\n",
       "      <th>target</th>\n",
       "      <th>review_body_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4577737</th>\n",
       "      <td>4</td>\n",
       "      <td>First off, the scent.&lt;br /&gt; It's excellent. Th...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>first off the scent it is excellent the aroma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912614</th>\n",
       "      <td>4</td>\n",
       "      <td>This is my first nose trimmer and I like this ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>this is my first nose trimmer and i like this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270775</th>\n",
       "      <td>4</td>\n",
       "      <td>After trying so many different acne products, ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>after trying so many different acne products i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243609</th>\n",
       "      <td>5</td>\n",
       "      <td>I had noticed that my temporal hairlines were ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i had noticed that my temporal hairlines were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309975</th>\n",
       "      <td>5</td>\n",
       "      <td>i got my plates a few days ago and tried out t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i got my plates a few days ago and tried out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496393</th>\n",
       "      <td>5</td>\n",
       "      <td>I have been using this for about two weeks and...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have been using this for about two weeks and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70813</th>\n",
       "      <td>5</td>\n",
       "      <td>I have tried LOTS of lip balm, use it 20 times...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have tried lots of lip balm use it times a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055470</th>\n",
       "      <td>5</td>\n",
       "      <td>I tried other products but nothing works as we...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i tried other products but nothing works as we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131857</th>\n",
       "      <td>5</td>\n",
       "      <td>I have tried other stem cell treatments, costi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have tried other stem cell treatments costin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743912</th>\n",
       "      <td>5</td>\n",
       "      <td>I used this gel for the first time last night ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i used this gel for the first time last night ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  \\\n",
       "4577737           4  First off, the scent.<br /> It's excellent. Th...   \n",
       "4912614           4  This is my first nose trimmer and I like this ...   \n",
       "3270775           4  After trying so many different acne products, ...   \n",
       "4243609           5  I had noticed that my temporal hairlines were ...   \n",
       "4309975           5  i got my plates a few days ago and tried out t...   \n",
       "2496393           5  I have been using this for about two weeks and...   \n",
       "70813             5  I have tried LOTS of lip balm, use it 20 times...   \n",
       "3055470           5  I tried other products but nothing works as we...   \n",
       "131857            5  I have tried other stem cell treatments, costi...   \n",
       "1743912           5  I used this gel for the first time last night ...   \n",
       "\n",
       "         star_rating_numeric   target  \\\n",
       "4577737                  4.0  class_3   \n",
       "4912614                  4.0  class_3   \n",
       "3270775                  4.0  class_3   \n",
       "4243609                  5.0  class_3   \n",
       "4309975                  5.0  class_3   \n",
       "2496393                  5.0  class_3   \n",
       "70813                    5.0  class_3   \n",
       "3055470                  5.0  class_3   \n",
       "131857                   5.0  class_3   \n",
       "1743912                  5.0  class_3   \n",
       "\n",
       "                                            review_body_pp  \n",
       "4577737  first off the scent it is excellent the aroma ...  \n",
       "4912614  this is my first nose trimmer and i like this ...  \n",
       "3270775  after trying so many different acne products i...  \n",
       "4243609  i had noticed that my temporal hairlines were ...  \n",
       "4309975  i got my plates a few days ago and tried out t...  \n",
       "2496393  i have been using this for about two weeks and...  \n",
       "70813    i have tried lots of lip balm use it times a d...  \n",
       "3055470  i tried other products but nothing works as we...  \n",
       "131857   i have tried other stem cell treatments costin...  \n",
       "1743912  i used this gel for the first time last night ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_df = read_data('amazon_reviews_us_Beauty_v1_00.tsv')\n",
    "tiny_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare vectors\n",
    "# TODO: Train own word2vec model\n",
    "word_vec = gs.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - King - Man + Woman = Queen\n",
      "['queen', 'monarch', 'princess', 'crown_prince', 'prince', 'kings', 'Queen_Consort', 'queens', 'sultan', 'monarchy']\n",
      "Test 2 - Plane - Air + Water = Boat\n",
      "['boat', 'engine_Cessna', 'Piper_Cherokee', 'radar_deflectors', 'pontoon_boat', 'airplane', 'plane_crashed', 'sailboat', 'canoe', 'desalinator']\n",
      "Test 3 - Outstanding ~ Excellent\n",
      "['oustanding', 'Outstanding', 'exceptional', 'anchorman_Jason_Lezak', 'outsanding', 'Stock_HEI', 'excellent', 'Synplicity_FPGA_implementation', 'exemplary', 'W3_Awards_honors']\n",
      "Test 4 - Cat - Kitten + Puppy = Dog\n",
      "['dog', 'pet', 'dogs', 'cats', 'pup', 'pooch', 'beagle', 'golden_retriever', 'puppies', 'dachshund']\n",
      "Test 5 - France - Paris + Tokyo = Japan\n",
      "['japan', 'hong_kong', 'japanese', 'seoul', 'germany', 'america', 'europe', 'latin_america', 'massachusetts', 'chinese']\n"
     ]
    }
   ],
   "source": [
    "# Test 1 - King - Man + Woman = Queen\n",
    "sims = word_vec.most_similar(\n",
    "    positive=['king', 'woman'],\n",
    "    negative=['man'])\n",
    "\n",
    "print(\"Test 1 - King - Man + Woman = Queen\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 2 - Plane - Air + Water = Boat\n",
    "sims = word_vec.most_similar(\n",
    "    positive=['plane', 'water'],\n",
    "    negative=['air'])\n",
    "\n",
    "print(\"Test 2 - Plane - Air + Water = Boat\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 3 - Outstanding ~ Excellent\n",
    "sims = word_vec.most_similar(positive=['outstanding'])\n",
    "\n",
    "print(\"Test 3 - Outstanding ~ Excellent\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 4 - Cat - Kitten + Puppy = Dog\n",
    "sims = word_vec.most_similar(\n",
    "    positive=['cat', 'puppy'],\n",
    "    negative=['kitten'])\n",
    "\n",
    "print(\"Test 4 - Cat - Kitten + Puppy = Dog\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 5 - France - Paris + Tokyo = Japan\n",
    "sims = word_vec.most_similar(\n",
    "    positive=['france', 'tokyo'],\n",
    "    negative=['paris'])\n",
    "\n",
    "print(\"Test 5 - France - Paris + Tokyo = Japan\")\n",
    "print([word for word, _ in sims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - King - Man + Woman = Queen\n",
      "['suppositories', 'simplicity', 'cytochrome', 'ej', 'sable', 'grasping', 'gluconic', 'nouveau', 'clostebol', 'promag']\n",
      "Test 2 - Plane - Air + Water = Boat\n",
      "['kitchen', 'cup', 'refilling', 'tub', 'faucet', 'fridge', 'tablet', 'keyboard', 'cabinet', 'brine']\n",
      "Test 3 - Outstanding ~ Excellent\n",
      "['excellent', 'exceptional', 'expedient', 'amway', 'inferior', 'incredible', 'acceptable', 'avid', 'authorized', 'authentic']\n",
      "Test 4 - Cat - Kitten + Puppy = Dog\n",
      "['tie', 'pinned', 'ears', 'nest', 'crying', 'creeping', 'spine', 'shirt', 'hairline', 'rolls']\n",
      "Test 5 - France - Paris + Tokyo = Japan\n",
      "['flowers', 'china', 'luxe', 'headbands', 'wif', 'incense', 'bows', 'scents', 'japan', 'germany']\n"
     ]
    }
   ],
   "source": [
    "my_word_vec = Word2Vec(\n",
    "    sentences=[i.split() for i in tiny_df.review_body_pp.to_list()],\n",
    "    window=13,\n",
    "    vector_size=300,\n",
    "    min_count=1\n",
    ")\n",
    "\n",
    "\n",
    "# Test 1 - King - Man + Woman = Queen\n",
    "sims = my_word_vec.wv.most_similar(\n",
    "    positive=['king', 'woman'],\n",
    "    negative=['man'])\n",
    "\n",
    "print(\"Test 1 - King - Man + Woman = Queen\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 2 - Plane - Air + Water = Boat/Ship\n",
    "sims = my_word_vec.wv.most_similar(\n",
    "    positive=['plane', 'water'],\n",
    "    negative=['air'])\n",
    "\n",
    "print(\"Test 2 - Plane - Air + Water = Boat\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 3 - Outstanding ~ Excellent\n",
    "sims = my_word_vec.wv.most_similar(positive=['outstanding'])\n",
    "\n",
    "print(\"Test 3 - Outstanding ~ Excellent\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 4 - Cat - Kitten + Puppy = Dog\n",
    "sims = my_word_vec.wv.most_similar(\n",
    "    positive=['cat', 'puppy'],\n",
    "    negative=['kitten'])\n",
    "\n",
    "print(\"Test 4 - Cat - Kitten + Puppy = Dog\")\n",
    "print([word for word, _ in sims])\n",
    "\n",
    "# Test 5 - France - Paris + Tokyo = Japan\n",
    "sims = my_word_vec.wv.most_similar(\n",
    "    positive=['france', 'tokyo'],\n",
    "    negative=['paris'])\n",
    "\n",
    "print(\"Test 5 - France - Paris + Tokyo = Japan\")\n",
    "print([word for word, _ in sims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of the above models**\n",
    "\n",
    "* As it can be seen from the outputs shown above, the pre-trained word2vec model encodes word similarities much better than the model trained on the Amazon reviews dataset\n",
    "* One of the reasons for this could be the size of the dataset. The amazon reviews is significantly smaller as compared to the google news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings():\n",
    "    \n",
    "    # Generate average embedding\n",
    "    X_average_features = np.vstack(tiny_df.review_body_pp.apply(\n",
    "        get_sentence_embedding, args=(word_vec, 'average')).to_numpy())\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_average_features, \n",
    "                                                        tiny_df.target.to_numpy(),\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Save above arrays to disk\n",
    "    with open('average_embedding.npz', 'wb') as file:\n",
    "        np.savez(\n",
    "            file=file,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test\n",
    "        )\n",
    "        \n",
    "    print(\"Saved average embeddings\")\n",
    "\n",
    "    # Generate truncated features with length 10\n",
    "    X_truncated_features = np.stack(tiny_df.review_body_pp.apply(\n",
    "        get_sentence_embedding, args=(word_vec, 'truncate', 10, True)).to_numpy())\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_truncated_features, \n",
    "                                                        tiny_df.target.to_numpy(),\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Save above arrays to disk\n",
    "    with open('truncated_embedding_10.npz', 'wb') as file:\n",
    "        np.savez(\n",
    "            file=file,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test\n",
    "        )\n",
    "        \n",
    "    print(\"Saved truncated embeddings - 10\")\n",
    "    \n",
    "    # Generate truncated features with length 20\n",
    "    X_truncated_features = np.stack(tiny_df.review_body_pp.apply(\n",
    "        get_sentence_embedding, args=(word_vec, 'truncate', 20, False)).to_numpy())\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_truncated_features, \n",
    "                                                        tiny_df.target.to_numpy(),\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Save above arrays to disk\n",
    "    with open('truncated_embedding_20.npz', 'wb') as file:\n",
    "        np.savez(\n",
    "            file=file,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test\n",
    "        )\n",
    "    \n",
    "    print(\"Saved truncated embeddings - 20\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved average embeddings\n",
      "Saved truncated embeddings - 10\n",
      "Saved truncated embeddings - 20\n"
     ]
    }
   ],
   "source": [
    "save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the word2vec object and dataframe to save memory\n",
    "try:\n",
    "    del word_vec\n",
    "    del my_word_vec\n",
    "    del tiny_df\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you run into memory issues, please restart the kernel and run the below code after you run the imports, utility functions and models section of this notebook.\n",
    "\n",
    "The following steps have been performed at this point\n",
    "* Dataset preprocessing - reading, cleaning and selecting 60k reviews\n",
    "* Feature extraction - 3 sets of features (average, truncated w/ length 10, truncated w/ length 20) have been extracted and saved to `.npz` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Simple models\n",
    "\n",
    "* Perceptron\n",
    "* Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Perceptron model: 0.63475\n",
      "Accuracy using SVM model: 0.6550833333333334\n"
     ]
    }
   ],
   "source": [
    "def part_3():\n",
    "    # Read data from npz files\n",
    "    data = np.load('./average_embedding.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Train a perceptron model\n",
    "    perceptron = Perceptron(eta0=1.5)\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    accuracy = get_model_metrics(perceptron, X_test, y_test)\n",
    "    print(f\"Accuracy using Perceptron model: {accuracy}\")\n",
    "    \n",
    "    # Train SVM model\n",
    "    svc = LinearSVC(penalty='l2', max_iter=1000)\n",
    "    svc.fit(X_train, y_train)\n",
    "    accuracy = get_model_metrics(svc, X_test, y_test)\n",
    "    print(f\"Accuracy using SVM model: {accuracy}\")\n",
    "\n",
    "\n",
    "part_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracies of Perceptron and SVM trained using TFIDF/Word2Vec Features**\n",
    "\n",
    "\n",
    "|            | TFIDF(%) | Word2Vec(%) |\n",
    "|:----------:|:--------:|:-----------:|\n",
    "| Perceptron |   69.77  |    63.47    |\n",
    "|     SVM    |   73.4   |    65.50    |\n",
    "\n",
    "* Although we expect the model trained on Word2Vec features to work better as compared to the one trained on TFIDF features, we can see that the performance of TF-IDF models is better in this case\n",
    "* This could be attributed to the fact that the TF-IDF features are trained on just the amazon reviews dataset whereas the pretrained Word2Vec model was trained on a much larger dataset. Hence, TF-IDF was a better option since it was better suited for the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 382.55it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1026.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.02\t0.47\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 467.81it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1047.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.63\n",
      "Valid\t0.01\t0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 466.15it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1066.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.64\n",
      "Valid\t0.01\t0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 466.67it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1064.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.64\n",
      "Valid\t0.01\t0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 465.49it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1084.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 463.41it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1066.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 463.58it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1069.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 467.24it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1061.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 466.63it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1034.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:01<00:00, 473.43it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1055.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.65\n",
      "Accuracy of FNN model trained on average vectors: 0.6545877456665039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def part_4_a():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    vector_size = 300\n",
    "    num_classes = 3\n",
    "    \n",
    "    model = FNN(\n",
    "        input_size=vector_size, \n",
    "        output_size=num_classes\n",
    "    )\n",
    "    \n",
    "    data = np.load('./average_embedding.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model, metrics = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optim,\n",
    "        criterion=criterion,\n",
    "        num_epochs=10\n",
    "    )\n",
    "    \n",
    "    # torch.save(model.state_dict(), './saved_models/4a.pth')\n",
    "    \n",
    "    print(f\"Accuracy of FNN model trained on average vectors: {metrics['val_acc']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "_ = part_4_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 3000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 358.57it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 659.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.02\t0.49\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 364.73it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 658.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 363.63it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 656.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 365.75it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 643.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 361.29it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 655.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 364.96it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 663.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 369.40it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 639.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 361.84it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 648.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 364.54it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 644.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:02<00:00, 368.70it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 648.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.02\t0.52\n",
      "Accuracy of FNN model trained on truncated vectors: 0.5221908092498779\n"
     ]
    }
   ],
   "source": [
    "def part_4_b():\n",
    "    vector_size = 3000\n",
    "    num_classes = 3\n",
    "    \n",
    "    model = FNN(\n",
    "        input_size=vector_size, \n",
    "        output_size=num_classes\n",
    "    )\n",
    "    \n",
    "    data = np.load('./truncated_embedding_10.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = StepLR(optim, 1, 0.05)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model, metrics = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optim,\n",
    "        criterion=criterion,\n",
    "        num_epochs=10,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy of FNN model trained on truncated vectors: {metrics['val_acc']}\")\n",
    "    \n",
    "    # torch.save(model.state_dict(), './saved_models/4b.pth')\n",
    "    return model\n",
    "\n",
    "\n",
    "_ = part_4_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracies of FNN models trained on Word2Vec features**\n",
    "\n",
    "|     | Average Features(%) | First 10 Words(%) |\n",
    "|:---:|:-------------------:|:-----------------:|\n",
    "| FNN |        65.45        |       52.21       |\n",
    "\n",
    "* It can be seen that the performance of the model trained on features generated by the average of all words in the sentence performed better\n",
    "* This could be due to the reason that we are considering all the words in the sentence to calculate the average whereas we are considering only the fist 10 words in the later case\n",
    "* It was also observed that the second model was overfitting to the training data and hence resulted in poor performance on the testing data. To mitigate for the overfitting problem, I used a learning rate scheduler for the second FNN\n",
    "\n",
    "* As compared with the simple models, we can see that FNN outperforms the perceptron model and achieves similar performance to the SVM model\n",
    "* Since there are significantly higher degrees of freedom in a FNN as compared to the perceptron, the model has higher chances of fitting to the training data better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (5) - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 176.77it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 466.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.41\n",
      "Valid\t0.02\t0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.97it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.50\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 186.61it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 461.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.52\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.29it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.53\n",
      "Valid\t0.02\t0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 183.75it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 463.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.53\n",
      "Valid\t0.01\t0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 186.14it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 471.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.54\n",
      "Valid\t0.01\t0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.90it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.54\n",
      "Valid\t0.01\t0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 184.55it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 461.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.55\n",
      "Valid\t0.01\t0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 182.97it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 474.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.55\n",
      "Valid\t0.01\t0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.70it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 465.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.56\n",
      "Valid\t0.01\t0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 184.05it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 464.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.82it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 472.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.01it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 465.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.58\n",
      "Valid\t0.01\t0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 184.79it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 469.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.58\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 183.60it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 463.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.58\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.43it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 465.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.67it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 468.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 186.52it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 474.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.94it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 461.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:02<00:00, 185.73it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 468.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.58\n",
      "Accuracy of RNN model: 0.5826961398124695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def part_5_a():\n",
    "    vector_size = 300\n",
    "    num_classes = 3\n",
    "    \n",
    "    model = RNNModel(\n",
    "        input_size=vector_size,\n",
    "        hidden_size=20,\n",
    "        num_layers=1,\n",
    "        output_size=num_classes\n",
    "    )\n",
    "    \n",
    "    data = np.load('./truncated_embedding_20.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model, metrics = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optim,\n",
    "        criterion=criterion,\n",
    "        num_epochs=20\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy of RNN model: {metrics['val_acc']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "_ = part_5_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of FNN and RNN**\n",
    "\n",
    "* FNN trained on average features outperforms RNN in the task at hand\n",
    "* This could be due to a similar reason that the FNN was trained on features generated from all the words as compared to the RNN that was trained only on the first 20 words\n",
    "* RNNs are expected to perform better in scenarios that involve long term dependencies. Given that most of the amazon reviews are not very long, we might run into the risk of using a model that is an overkill for the current task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 224.99it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 466.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.02\t0.50\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 226.86it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 467.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 225.60it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 473.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.61\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.44it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 461.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.62\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.74it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 457.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.63\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 225.07it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 459.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.64\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.40it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 463.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.64\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.59it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 472.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 224.17it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.89it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 458.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 225.29it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 469.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 224.33it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 459.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 222.37it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 462.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.67\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.43it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 466.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.67\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 225.34it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 225.01it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 482.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.87it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 463.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.63it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 464.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.69\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.32it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 458.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.69\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 222.72it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 453.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.69\n",
      "Valid\t0.01\t0.61\n",
      "Accuracy of GRU model: 0.611951470375061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def part_5_b():\n",
    "    vector_size = 300\n",
    "    num_classes = 3\n",
    "    \n",
    "    model = RNNModel(\n",
    "        input_size=vector_size,\n",
    "        hidden_size=20,\n",
    "        num_layers=2,\n",
    "        output_size=num_classes,\n",
    "        rnn_layer_type='gru'\n",
    "    )\n",
    "    \n",
    "    data = np.load('./truncated_embedding_20.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model, metrics = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optim,\n",
    "        criterion=criterion,\n",
    "        num_epochs=20\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy of GRU model: {metrics['val_acc']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "_ = part_5_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.48it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 454.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.02\t0.48\n",
      "Valid\t0.01\t0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.00it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 470.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.57\n",
      "Valid\t0.01\t0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.55it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 462.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.59\n",
      "Valid\t0.01\t0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.25it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 459.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.61\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 213.15it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 454.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.62\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 219.11it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 457.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.63\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 218.90it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 453.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.63\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.46it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 456.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.64\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.11it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 456.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 221.39it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 459.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.65\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 219.58it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 450.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 219.41it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 452.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.66\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.88it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 453.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.67\n",
      "Valid\t0.01\t0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 218.48it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 450.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.67\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 219.92it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 461.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.67\n",
      "Valid\t0.01\t0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 216.85it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 451.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 219.13it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 451.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 223.84it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 455.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.68\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.84it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 455.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.69\n",
      "Valid\t0.01\t0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 220.87it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 462.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20\n",
      "Mode\tLoss\tAcc\n",
      "Train\t0.01\t0.69\n",
      "Valid\t0.01\t0.61\n",
      "Accuracy of LSTM model: 0.6144447922706604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def part_5_c():\n",
    "    vector_size = 300\n",
    "    num_classes = 3\n",
    "    \n",
    "    model = RNNModel(\n",
    "        input_size=vector_size,\n",
    "        hidden_size=20,\n",
    "        num_layers=2,\n",
    "        output_size=num_classes,\n",
    "        rnn_layer_type='lstm'\n",
    "    )\n",
    "    \n",
    "    data = np.load('./truncated_embedding_20.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model, metrics = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optim,\n",
    "        criterion=criterion,\n",
    "        num_epochs=20\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy of LSTM model: {metrics['val_acc']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "_ = part_5_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of RNN, GRU and LSTM**\n",
    "\n",
    "|      | Accuracy(%) |\n",
    "|:----:|:-----------:|\n",
    "|  RNN |    58.26    |\n",
    "|  GRU |    61.19    |\n",
    "| LSTM |    61.44    |\n",
    "\n",
    "* As it can be seen from the above table, LSTM performs better than RNN and GRU\n",
    "* RNNs also suffer from the problem of vanishing gradients which leads to smaller weight updates as epochs progress. LSTMs address this issue\n",
    "* LSTMs also have the ability to forget irrelevant information using the \"forget gate\". This could also be the reason for the improved performance of LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusion\n",
    "\n",
    "|            Model           | Accuracy(%) |\n",
    "|:--------------------------:|:-----------:|\n",
    "|         Perceptron         |    63.47    |\n",
    "|           **SVM**          |  **65.50**  |\n",
    "| **FNN (Average features)** |  **65.45**  |\n",
    "|  FNN (Truncated features)  |    52.21    |\n",
    "|             RNN            |    58.26    |\n",
    "|             GRU            |    61.19    |\n",
    "|            LSTM            |    61.44    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d882ba3a4378c595baf16ee82511dae009226ab41a91269322ba630fc2f7752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
