{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Below steps were performed to clean reviews\n",
    "\n",
    "1. Convert text to lower case\n",
    "2. Remove HTML tags and URLs from reviews using regex substitutions\n",
    "3. Expand contractions using [`contractions`](https://pypi.org/project/contractions/) library\n",
    "4. Remove non-alphabetical characters using regex substitutions\n",
    "5. Substitute multiple white spaces with one white space\n",
    "\n",
    "Steps 3 was performed before step 4 since removing non-alphabetical characters would interfere with contractions such as \"won't\". Hence, contractions were performed before removing non-alphabetical characters.\n",
    "\n",
    "* Average length of reviews before cleaning:    271.9\n",
    "* Average length of reviews after cleaning:     262.4\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "Preprocessing of text involves\n",
    "\n",
    "1. Removing stop words\n",
    "2. Lemmatizing words\n",
    "    * Tokenize words\n",
    "    * Part of speech tagging\n",
    "    * Lemmatizing using the POS tags\n",
    "\n",
    "It was observed that the performance of all the models was better when stop words were not removed. This could be because of the increased number of features extracted from the review body. Results from both methods are attached later in the report.\n",
    "\n",
    "Since, we observed that the performance of models trained on text **without stopwords removal** is better, the same was submitted for final evaluation.\n",
    "\n",
    "* Average length of reviews before preprocessing:    262.4\n",
    "* Average length of reviews after preprocessing:     251.4\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "We used `sklearn.feature_extraction.text.TfidfVectorizer` to extract features from reviews\n",
    "\n",
    "* Grid search along with cross validation was used to fix the parameters of TFIDF\n",
    "* After grid search, I found that using n grams yields better results across all models. Hence, I set the `ngram_range` parameter to `(1, 2)`. This extracts features consisting of 1 word and 2 words resulting in increased number of features\n",
    "* By performing grid search, it was also found that using all the features extracted by TFIDF resulted in maximum average precision\n",
    "\n",
    "# Perceptron\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* `eta0`: 0.001\n",
    "* `penalty`: l2\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* `C`: 1.0\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "**Hyperparameters**: Default parameters\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "\n",
    "**Hyperparameters**: Default parameters\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "## With Stopwords removal\n",
    "\n",
    "![](with_stopwords.png)\n",
    "\n",
    "## Without Stopwords removal\n",
    "![](without_stopwords_removal.png)\n",
    "\n",
    "* As it can be seen from the above results, the performance of all the models is better when stopwords aren't removed\n",
    "* Logistic regression is performing better on this dataset as compared to the other models\n",
    "\n",
    "# References\n",
    "\n",
    "* [Treebank Part of Speech Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aditya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/aditya/anaconda3/envs/ee641/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/aditya/anaconda3/envs/ee641/lib/python3.10/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/aditya/anaconda3/envs/ee641/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og = pd.read_csv('amazon_reviews_us_Beauty_v1_00.tsv',\n",
    "                      delimiter='\\t',\n",
    "                      usecols=['star_rating', 'review_body'],\n",
    "                      on_bad_lines='skip',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_body\n",
       "0           5                   Love this, excellent sun block!!\n",
       "1           5  The great thing about this cream is that it do...\n",
       "2           5  Great Product, I'm 65 years old and this is al...\n",
       "3           5  I use them as shower caps & conditioning caps....\n",
       "4           5  This is my go-to daily sunblock. It leaves no ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performed when reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_column(column, bins, labels):\n",
    "    \"\"\"\n",
    "    :param column: pd.Series\n",
    "    :param bins: list\n",
    "    :param labels: list\n",
    "    :return: pd.Series\n",
    "    \"\"\"\n",
    "\n",
    "    bins.insert(0, -float('inf'))\n",
    "\n",
    "    # Use pd.IntervalIndex to create bins to split the data\n",
    "    bins = pd.IntervalIndex.from_breaks(bins)\n",
    "\n",
    "    print(bins)\n",
    "\n",
    "    x = pd.cut(column, bins=bins, include_lowest=True)\n",
    "    x = x.cat.rename_categories(labels)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    # Convert ratings to numeric\n",
    "    # Ignore ratings that are not numerals\n",
    "    dataframe['star_rating_numeric'] = pd.to_numeric(dataframe.star_rating, errors='coerce')\n",
    "\n",
    "    # Drop NaN\n",
    "    dataframe.dropna(inplace=True)\n",
    "\n",
    "    # Bin ratings into 3 classes\n",
    "    # 1 and 2   class_1\n",
    "    # 3         class_2\n",
    "    # 4 and 5   class_3\n",
    "\n",
    "    dataframe['target'] = bin_column(dataframe.star_rating_numeric,\n",
    "                                     [2, 3, 5],\n",
    "                                     labels=['class_1', 'class_2', 'class_3'])\n",
    "\n",
    "    # In the interest of computational simplicity,\n",
    "    # keep only 20000 instances of each class\n",
    "\n",
    "    tiny_df = pd.DataFrame(\n",
    "        columns=['star_rating', 'review_body', 'star_rating_numeric'])\n",
    "\n",
    "    for cls in dataframe.target.unique():\n",
    "        tiny_df = pd.concat([\n",
    "            tiny_df,\n",
    "            dataframe[dataframe.target == cls].sample(20000, random_state=42)\n",
    "        ])\n",
    "\n",
    "    return tiny_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalIndex([(-inf, 2.0], (2.0, 3.0], (3.0, 5.0]], dtype='interval[float64, right]')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating_numeric</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1797264</th>\n",
       "      <td>4</td>\n",
       "      <td>Around 25% less than buying at the store + fre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951025</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it! Clean, fresh, seems greasy at first, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821987</th>\n",
       "      <td>5</td>\n",
       "      <td>This color is beautiful and amazing for summer...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666293</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been on the hunt for the perfect lotion. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776503</th>\n",
       "      <td>5</td>\n",
       "      <td>my son has eczema since he was born and his sk...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551303</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the classiest polish I have ever seen....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459989</th>\n",
       "      <td>4</td>\n",
       "      <td>it's not very pigmented but that was why I bou...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725454</th>\n",
       "      <td>5</td>\n",
       "      <td>Yay, my new favorite cleanser! I've been using...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080912</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I ordered this on 12 Nov 2014, and received it...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282098</th>\n",
       "      <td>5</td>\n",
       "      <td>I  have purchased so many new styling tools tr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  \\\n",
       "1797264           4  Around 25% less than buying at the store + fre...   \n",
       "2951025           5  Love it! Clean, fresh, seems greasy at first, ...   \n",
       "2821987           5  This color is beautiful and amazing for summer...   \n",
       "2666293           5  I've been on the hunt for the perfect lotion. ...   \n",
       "2776503           5  my son has eczema since he was born and his sk...   \n",
       "3551303           5  This is the classiest polish I have ever seen....   \n",
       "1459989           4  it's not very pigmented but that was why I bou...   \n",
       "725454            5  Yay, my new favorite cleanser! I've been using...   \n",
       "2080912         5.0  I ordered this on 12 Nov 2014, and received it...   \n",
       "2282098           5  I  have purchased so many new styling tools tr...   \n",
       "\n",
       "         star_rating_numeric   target  \n",
       "1797264                  4.0  class_3  \n",
       "2951025                  5.0  class_3  \n",
       "2821987                  5.0  class_3  \n",
       "2666293                  5.0  class_3  \n",
       "2776503                  5.0  class_3  \n",
       "3551303                  5.0  class_3  \n",
       "1459989                  4.0  class_3  \n",
       "725454                   5.0  class_3  \n",
       "2080912                  5.0  class_3  \n",
       "2282098                  5.0  class_3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_df = prepare_data(data_og.copy())\n",
    "tiny_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of review before cleaning: 271.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:02<00:00, 29004.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of review before cleaning: 262.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CLEAN_HTML = re.compile('<.*?>')            # Regex to match HTML tags\n",
    "CLEAN_URL = re.compile('(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "CLEAN_SPACES = re.compile('\\s+')            # Regex to match multiple spaces\n",
    "CLEAN_NON_ALPHA = re.compile('[^a-zA-Z]')   # Regex to match non-alphabetic characters\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML and URL tags from text\n",
    "    text = re.sub(CLEAN_HTML, ' ', text)\n",
    "    text = re.sub(CLEAN_URL, ' ', text)\n",
    "\n",
    "    # Perform contractions on the text\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(CLEAN_NON_ALPHA, ' ', text)\n",
    "\n",
    "    # Remove additional spaces\n",
    "    text = re.sub(CLEAN_SPACES, ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "print(f\"Average length of review before cleaning: {tiny_df.review_body.apply(len).mean()}\")\n",
    "tiny_df['review_body_pp'] = tiny_df.review_body.progress_apply(clean_text)\n",
    "print(f\"Average length of review before cleaning: {tiny_df.review_body_pp.apply(len).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating_numeric</th>\n",
       "      <th>target</th>\n",
       "      <th>review_body_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1797264</th>\n",
       "      <td>4</td>\n",
       "      <td>Around 25% less than buying at the store + fre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>around less than buying at the store free ship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951025</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it! Clean, fresh, seems greasy at first, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>love it clean fresh seems greasy at first but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821987</th>\n",
       "      <td>5</td>\n",
       "      <td>This color is beautiful and amazing for summer...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>this color is beautiful and amazing for summer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666293</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been on the hunt for the perfect lotion. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have been on the hunt for the perfect lotion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776503</th>\n",
       "      <td>5</td>\n",
       "      <td>my son has eczema since he was born and his sk...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>my son has eczema since he was born and his sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551303</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the classiest polish I have ever seen....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>this is the classiest polish i have ever seen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459989</th>\n",
       "      <td>4</td>\n",
       "      <td>it's not very pigmented but that was why I bou...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>it is not very pigmented but that was why i bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725454</th>\n",
       "      <td>5</td>\n",
       "      <td>Yay, my new favorite cleanser! I've been using...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>yay my new favorite cleanser i have been using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080912</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I ordered this on 12 Nov 2014, and received it...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i ordered this on nov and received it november...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282098</th>\n",
       "      <td>5</td>\n",
       "      <td>I  have purchased so many new styling tools tr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have purchased so many new styling tools try...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  \\\n",
       "1797264           4  Around 25% less than buying at the store + fre...   \n",
       "2951025           5  Love it! Clean, fresh, seems greasy at first, ...   \n",
       "2821987           5  This color is beautiful and amazing for summer...   \n",
       "2666293           5  I've been on the hunt for the perfect lotion. ...   \n",
       "2776503           5  my son has eczema since he was born and his sk...   \n",
       "3551303           5  This is the classiest polish I have ever seen....   \n",
       "1459989           4  it's not very pigmented but that was why I bou...   \n",
       "725454            5  Yay, my new favorite cleanser! I've been using...   \n",
       "2080912         5.0  I ordered this on 12 Nov 2014, and received it...   \n",
       "2282098           5  I  have purchased so many new styling tools tr...   \n",
       "\n",
       "         star_rating_numeric   target  \\\n",
       "1797264                  4.0  class_3   \n",
       "2951025                  5.0  class_3   \n",
       "2821987                  5.0  class_3   \n",
       "2666293                  5.0  class_3   \n",
       "2776503                  5.0  class_3   \n",
       "3551303                  5.0  class_3   \n",
       "1459989                  4.0  class_3   \n",
       "725454                   5.0  class_3   \n",
       "2080912                  5.0  class_3   \n",
       "2282098                  5.0  class_3   \n",
       "\n",
       "                                            review_body_pp  \n",
       "1797264  around less than buying at the store free ship...  \n",
       "2951025  love it clean fresh seems greasy at first but ...  \n",
       "2821987  this color is beautiful and amazing for summer...  \n",
       "2666293  i have been on the hunt for the perfect lotion...  \n",
       "2776503  my son has eczema since he was born and his sk...  \n",
       "3551303  this is the classiest polish i have ever seen ...  \n",
       "1459989  it is not very pigmented but that was why i bo...  \n",
       "725454   yay my new favorite cleanser i have been using...  \n",
       "2080912  i ordered this on nov and received it november...  \n",
       "2282098  i have purchased so many new styling tools try...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of review before preprocessing: 262.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:14<00:00, 808.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of review before preprocessing: 251.48775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    treebank_to_wordnet = {\n",
    "        'n': 'n', # Noun\n",
    "        'v': 'v', # Verb,\n",
    "        'j': 'a', # Adjective\n",
    "        'r': 'r', # Adverb\n",
    "    }\n",
    "    wordnet_tag = treebank_to_wordnet.get(treebank_tag[0].lower())\n",
    "    return wordnet_tag if wordnet_tag is not None else 'n'\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    * Remove stop words\n",
    "    * Perform lemmatization\n",
    "    :param text: str\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    # Get a list of all the words\n",
    "    words = text.split(' ')\n",
    "\n",
    "    # Remove stop words\n",
    "    # words = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "    # Tokenize the rest of the text using NLTK\n",
    "    text = ' '.join(words)\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    # Get part of speech tags and use it for lemmatization\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    # print(pos_tags)\n",
    "    # print([get_wordnet_pos(v) for _, v in pos_tags])\n",
    "    words = [lemmatizer.lemmatize(word=w, pos=get_wordnet_pos(v)) for w, v in pos_tags]\n",
    "    return ' '.join(words)\n",
    "\n",
    "tqdm.pandas()\n",
    "print(f\"Average length of review before preprocessing: {tiny_df.review_body_pp.apply(len).mean()}\")\n",
    "tiny_df['review_body_pp'] = tiny_df.review_body_pp.progress_apply(preprocess_text)\n",
    "print(f\"Average length of review before preprocessing: {tiny_df.review_body_pp.apply(len).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating_numeric</th>\n",
       "      <th>target</th>\n",
       "      <th>review_body_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1797264</th>\n",
       "      <td>4</td>\n",
       "      <td>Around 25% less than buying at the store + fre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>around less than buying at the store free ship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951025</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it! Clean, fresh, seems greasy at first, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>love it clean fresh seem greasy at first but o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821987</th>\n",
       "      <td>5</td>\n",
       "      <td>This color is beautiful and amazing for summer...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>this color be beautiful and amaze for summer t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666293</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been on the hunt for the perfect lotion. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have be on the hunt for the perfect lotion t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776503</th>\n",
       "      <td>5</td>\n",
       "      <td>my son has eczema since he was born and his sk...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>my son have eczema since he be bear and his sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551303</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the classiest polish I have ever seen....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>this be the classy polish i have ever see it l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459989</th>\n",
       "      <td>4</td>\n",
       "      <td>it's not very pigmented but that was why I bou...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>it be not very pigmented but that be why i buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725454</th>\n",
       "      <td>5</td>\n",
       "      <td>Yay, my new favorite cleanser! I've been using...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>yay my new favorite cleanser i have be use an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080912</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I ordered this on 12 Nov 2014, and received it...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i order this on nov and receive it november da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282098</th>\n",
       "      <td>5</td>\n",
       "      <td>I  have purchased so many new styling tools tr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>class_3</td>\n",
       "      <td>i have purchase so many new styling tool try t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  \\\n",
       "1797264           4  Around 25% less than buying at the store + fre...   \n",
       "2951025           5  Love it! Clean, fresh, seems greasy at first, ...   \n",
       "2821987           5  This color is beautiful and amazing for summer...   \n",
       "2666293           5  I've been on the hunt for the perfect lotion. ...   \n",
       "2776503           5  my son has eczema since he was born and his sk...   \n",
       "3551303           5  This is the classiest polish I have ever seen....   \n",
       "1459989           4  it's not very pigmented but that was why I bou...   \n",
       "725454            5  Yay, my new favorite cleanser! I've been using...   \n",
       "2080912         5.0  I ordered this on 12 Nov 2014, and received it...   \n",
       "2282098           5  I  have purchased so many new styling tools tr...   \n",
       "\n",
       "         star_rating_numeric   target  \\\n",
       "1797264                  4.0  class_3   \n",
       "2951025                  5.0  class_3   \n",
       "2821987                  5.0  class_3   \n",
       "2666293                  5.0  class_3   \n",
       "2776503                  5.0  class_3   \n",
       "3551303                  5.0  class_3   \n",
       "1459989                  4.0  class_3   \n",
       "725454                   5.0  class_3   \n",
       "2080912                  5.0  class_3   \n",
       "2282098                  5.0  class_3   \n",
       "\n",
       "                                            review_body_pp  \n",
       "1797264  around less than buying at the store free ship...  \n",
       "2951025  love it clean fresh seem greasy at first but o...  \n",
       "2821987  this color be beautiful and amaze for summer t...  \n",
       "2666293  i have be on the hunt for the perfect lotion t...  \n",
       "2776503  my son have eczema since he be bear and his sk...  \n",
       "3551303  this be the classy polish i have ever see it l...  \n",
       "1459989  it be not very pigmented but that be why i buy...  \n",
       "725454   yay my new favorite cleanser i have be use an ...  \n",
       "2080912  i order this on nov and receive it november da...  \n",
       "2282098  i have purchase so many new styling tool try t...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_features.fit(tiny_df.review_body_pp.values)\n",
    "\n",
    "# Training data after TF-IDF feature extraction\n",
    "X = tfidf_features.transform(tiny_df.review_body_pp.values)\n",
    "y = tiny_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aa battery', 'aa be', ..., 'zy event', 'zytaze',\n",
       "       'zytaze really'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_cv(pipeline, parameter_grid, training_data, target_labels):\n",
    "    cv = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=parameter_grid,\n",
    "    )\n",
    "    cv.fit(training_data, target_labels)\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_model_metrics(pipeline, testing_data, testing_labels, print_output=True):\n",
    "    y_pred = pipeline.predict(testing_data)\n",
    "    clf_report = classification_report(testing_labels, y_pred, output_dict=True)\n",
    "\n",
    "    if print_output:\n",
    "        for cls in set(testing_labels):\n",
    "            print(f\"{clf_report[cls]['precision']}, \"\n",
    "                  f\"{clf_report[cls]['recall']}, \"\n",
    "                  f\"{clf_report[cls]['f1-score']}\")\n",
    "        print(f\"{clf_report['weighted avg']['precision']}, \"\n",
    "              f\"{clf_report['weighted avg']['recall']}, \"\n",
    "              f\"{clf_report['weighted avg']['f1-score']}\")\n",
    "\n",
    "    # return clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Model\n",
      "0.602112676056338, 0.5950782997762863, 0.5985748218527316\n",
      "0.7889536356726627, 0.7857683573050719, 0.7873577749683945\n",
      "0.7027225901398086, 0.7137518684603886, 0.7081942899517982\n",
      "0.6974709192305503, 0.69775, 0.697588104198317\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron Model\")\n",
    "perceptron = fit_model_cv(Perceptron(),\n",
    "                          {'penalty': ['l1', 'l2'],\n",
    "                           'eta0': [1e-4, 0.001, 0.01, 0.1, 1]\n",
    "                           },\n",
    "                          X_train, y_train)\n",
    "\n",
    "get_model_metrics(perceptron, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine\n",
      "0.6535742340926944, 0.6204325130499627, 0.6365723029839327\n",
      "0.807205452775073, 0.836739843552864, 0.8217073472927766\n",
      "0.7353302234225386, 0.7461385151968112, 0.7406949425003091\n",
      "0.7316583224933828, 0.7339166666666667, 0.7325421742851563\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine\")\n",
    "svc = fit_model_cv(LinearSVC(),\n",
    "                   {'C': np.arange(1, 1.5, 0.2),},\n",
    "                   X_train, y_train)\n",
    "\n",
    "get_model_metrics(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "0.6606917445089624, 0.6505095699726572, 0.655561122244489\n",
      "0.822772027265842, 0.8223568004037345, 0.8225643614336193\n",
      "0.7442373712604218, 0.7561036372695565, 0.750123578843302\n",
      "0.7421647700377851, 0.7425833333333334, 0.7423450837190022\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Model\")\n",
    "logistic_reg = fit_model_cv(LogisticRegression(),\n",
    "                            {},\n",
    "                            # {'C': np.arange(1, 2, 0.2),},\n",
    "                            X_train, y_train)\n",
    "\n",
    "get_model_metrics(logistic_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Model\n",
      "0.604603370324702, 0.7312950534427044, 0.6619417257284285\n",
      "0.8702266376901583, 0.707292455210699, 0.7803452115812917\n",
      "0.7446971633018145, 0.7259591429995017, 0.735208780118582\n",
      "0.7391868281229881, 0.7215833333333334, 0.7255523066248427\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes Model\")\n",
    "naive_bayes = fit_model_cv(MultinomialNB(),\n",
    "                           {},\n",
    "                           X_train, y_train)\n",
    "\n",
    "get_model_metrics(naive_bayes, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
